{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import auto_compyute as ac\n",
    "import auto_compyute.nn.functional as F\n",
    "from auto_compyute import nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ac.backends.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_len = 256\n",
    "emb_dim = 384\n",
    "n_heads = 6\n",
    "n_blocks = 6\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_emb, emb_dim, seq_len, n_heads, n_layers, mask, dropout=0) -> None:\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(n_emb, emb_dim)\n",
    "        self.wpe = nn.Embedding(seq_len, emb_dim)\n",
    "        self.wte.w.data *= emb_dim**-0.5\n",
    "        self.wpe.w.data *= emb_dim**-0.5\n",
    "\n",
    "        out_scale = (2 * n_layers)**-0.5\n",
    "        self.blocks = nn.Modulelist(Block(emb_dim, n_heads, mask, dropout, out_scale) for _ in range(n_layers))\n",
    "\n",
    "        self.head_ln = nn.Layernorm((emb_dim))\n",
    "        self.head = nn.Linear(emb_dim, n_emb, bias=False)\n",
    "        self.head.w = self.wte.w\n",
    "\n",
    "        self.pos = nn.Buffer(ac.arange(seq_len).view((1, -1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.wte(x) + self.wpe(self.pos[:, : x.shape[-1]])\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(self.head_ln(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, mask, dropout, out_scale) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_ln = nn.Layernorm((emb_dim,))\n",
    "        self.attn = nn.MultiHeadSelfAttention(emb_dim, n_heads, mask, dropout)\n",
    "        self.attn.qkv.w.data *= out_scale\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_ln = nn.Layernorm((emb_dim,))\n",
    "        self.mlp = MLP(emb_dim)\n",
    "        self.mlp.down.w.data *= out_scale\n",
    "        self.mlp_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn_dropout(self.attn(self.attn_ln(x)))\n",
    "        x = x + self.mlp_dropout(self.mlp(self.mlp_ln(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_emb) -> None:\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(n_emb, 4*n_emb)\n",
    "        self.down = nn.Linear(4*n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    n_emb=256,\n",
    "    emb_dim=emb_dim,\n",
    "    seq_len=ctx_len,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_blocks,\n",
    "    mask=ac.full((ctx_len, ctx_len), float(\"-inf\")).triu(1)\n",
    ")\n",
    "model.to(ac.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ac.randi((batch_size, ctx_len), 0, 256, device=ac.cuda, dtype=ac.int32)\n",
    "y = ac.randi((batch_size, ctx_len), 0, 256, device=ac.cuda, dtype=ac.int32)\n",
    "loss = F.cross_entropy(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = ac.autograd.build_backward_queue(loss, [], set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "total_bytes = 0\n",
    "\n",
    "for node in queue:\n",
    "    print(node.ctx.name)\n",
    "    vals = node.ctx.cache.vals if node.ctx.cache.vals is not None else []\n",
    "    for v in vals:\n",
    "        v_id = id(v)\n",
    "        v_dtype = v.dtype if isinstance(v, ac.backends.Array) else type(v)\n",
    "        v_shape = v.shape if isinstance(v, ac.backends.Array) else 1\n",
    "        v_nbytes = v.nbytes if isinstance(v, ac.backends.Array) else sys.getsizeof(v)\n",
    "        total_bytes += v_nbytes\n",
    "        print(\"    \", v_id, v_dtype, v_shape, f\"{v_nbytes:_}\")\n",
    "\n",
    "print(f\"total {total_bytes:_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
