{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import auto_compyute as ac\n",
    "import auto_compyute.nn.functional as F\n",
    "from auto_compyute import nn\n",
    "\n",
    "ac.backends.set_random_seed(0)\n",
    "device = ac.cuda if ac.backends.gpu_available() else ac.cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_len = 256\n",
    "emb_dim = 384\n",
    "n_heads = 6\n",
    "n_blocks = 6\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# load data\n",
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(DATA_URL)\n",
    "data = response.text\n",
    "\n",
    "# tokenization\n",
    "chars = sorted(list(set(response.text)))\n",
    "vocab = {i: c for i, c in enumerate(chars)}\n",
    "ivocab = {c: i for i, c in vocab.items()}\n",
    "encode = lambda text: [ivocab[t] for t in text]\n",
    "decode = lambda token_ids: \"\".join(vocab[id] for id in token_ids)\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "data_enc = ac.tensor(encode(data), dtype=ac.int32)\n",
    "X = ac.stack(*[data_enc[i * ctx_len : i * ctx_len + ctx_len] for i in range(len(data_enc) // ctx_len)])\n",
    "y = ac.stack(*[data_enc[i * ctx_len + 1 : i * ctx_len + ctx_len + 1] for i in range(len(data_enc) // ctx_len)])\n",
    "n = int(len(X) * 0.9)\n",
    "X_train = X.int()[:n]\n",
    "y_train = y.int()[:n]\n",
    "X_val = X.int()[n:]\n",
    "y_val = y.int()[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_emb, emb_dim, seq_len, n_heads, n_layers, mask, dropout=0) -> None:\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(n_emb, emb_dim)\n",
    "        self.wpe = nn.Embedding(seq_len, emb_dim)\n",
    "        self.wte.w.data *= emb_dim**-0.5\n",
    "        self.wpe.w.data *= emb_dim**-0.5\n",
    "\n",
    "        out_scale = (2 * n_layers)**-0.5\n",
    "        self.blocks = nn.Modulelist(Block(emb_dim, n_heads, mask, dropout, out_scale) for _ in range(n_layers))\n",
    "\n",
    "        self.head_ln = nn.Layernorm((emb_dim))\n",
    "        self.head = nn.Linear(emb_dim, n_emb, bias=False)\n",
    "        self.head.w = self.wte.w\n",
    "\n",
    "        self.pos = nn.Buffer(ac.arange(seq_len).view((1, -1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.wte(x) + self.wpe(self.pos[:, : x.shape[-1]])\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(self.head_ln(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, mask, dropout, out_scale) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_ln = nn.Layernorm((emb_dim,))\n",
    "        self.attn = nn.MultiHeadSelfAttention(emb_dim, n_heads, mask, dropout)\n",
    "        self.attn.qkv.w.data *= out_scale\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_ln = nn.Layernorm((emb_dim,))\n",
    "        self.mlp = MLP(emb_dim)\n",
    "        self.mlp.down.w.data *= out_scale\n",
    "        self.mlp_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn_dropout(self.attn(self.attn_ln(x)))\n",
    "        x = x + self.mlp_dropout(self.mlp(self.mlp_ln(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_emb) -> None:\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(n_emb, 4*n_emb)\n",
    "        self.down = nn.Linear(4*n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    n_emb=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    seq_len=ctx_len,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_blocks,\n",
    "    mask=ac.full((ctx_len, ctx_len), float(\"-inf\")).triu(1)\n",
    ")\n",
    "model.to(ac.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = F.cross_entropy(model(X_train[:batch_size]), y_train[:batch_size])\n",
    "# ac.autograd.draw_compute_graph(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "train_dl = nn.Dataloader((X_train, y_train), batch_size, device)\n",
    "val_dl = nn.Dataloader((X_val, y_val), batch_size, device, False)\n",
    "optim = nn.optimizers.AdamW(model.parameters(), learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "step = 1\n",
    "max_steps = 2500\n",
    "val_interval = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "for step, (x, y) in enumerate(train_dl()):\n",
    "    start = time.perf_counter()\n",
    "    loss = F.cross_entropy(model(x), y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    dt = time.perf_counter() - start\n",
    "\n",
    "    tok_per_s = batch_size * ctx_len / dt\n",
    "    print(f\"step {step+1:4} | loss {loss.item():.4f} | dt {dt:.4f} s | {tok_per_s:.1f} tokens/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
