{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer: Shakespeare\n",
    "\n",
    "based on: https://github.com/karpathy/build-nanogpt\n",
    "\n",
    "In this example a transformer model is built close to the GPT-2 model. The building blocks are now implemented manually instead of using a `nn.Sequential` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import auto_compyute as ac\n",
    "import auto_compyute.nn.functional as F\n",
    "from auto_compyute import nn\n",
    "\n",
    "import time\n",
    "\n",
    "ac.backends.set_random_seed(0)\n",
    "device = \"cuda\" if ac.backends.gpu_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_len = 8  # number of tokens in the input sequence\n",
    "emb_dim = 64  # embedding dimension or model dimension\n",
    "n_heads = 4\n",
    "n_blocks = 6 # number of transformer blocks/layers\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# download data\n",
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(DATA_URL)\n",
    "data = response.text\n",
    "\n",
    "# create a character-level tokenizer\n",
    "chars = sorted(list(set(response.text)))\n",
    "vocab = {i: c for i, c in enumerate(chars)}\n",
    "ivocab = {c: i for i, c in vocab.items()}\n",
    "encode = lambda text: [ivocab[t] for t in text]\n",
    "decode = lambda token_ids: \"\".join(vocab[id] for id in token_ids)\n",
    "\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "data_enc = ac.tensor(encode(data), dtype=ac.int32)\n",
    "\n",
    "# create the datasets. y is just x shifted by one (the next token to predict)\n",
    "X = ac.stack(*[data_enc[i * ctx_len : i * ctx_len + ctx_len] for i in range(len(data_enc) // ctx_len)]).int()\n",
    "y = ac.stack(*[data_enc[i * ctx_len + 1 : i * ctx_len + ctx_len + 1] for i in range(len(data_enc) // ctx_len)]).int()\n",
    "\n",
    "print(\"X\")\n",
    "print(X[:4])\n",
    "print(\"y\")\n",
    "print(y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random train-val split\n",
    "idx = ac.randperm(len(X))\n",
    "n = int(len(X) * 0.9)\n",
    "train_idx, val_idx = idx[:n], idx[n:]\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_val = X[val_idx]\n",
    "y_val = y[val_idx]\n",
    "\n",
    "print(f'{X_train.shape=}')\n",
    "print(f'{y_train.shape=}')\n",
    "print(f'{X_val.shape=}')\n",
    "print(f'{y_val.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer model following GPT-2\"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, emb_dim, seq_len, n_heads, n_layers, mask, dropout=0):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(n_emb, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, emb_dim)\n",
    "        self.token_emb.w.data *= emb_dim**-0.5  # scaling used by GPT-2\n",
    "        self.pos_emb.w.data *= emb_dim**-0.5  # scaling used by GPT-2\n",
    "\n",
    "        out_scale = (2 * n_layers)**-0.5  # scaling used by GPT-2\n",
    "        self.blocks = nn.Modulelist(Block(emb_dim, n_heads, mask, dropout, out_scale) for _ in range(n_layers))\n",
    "\n",
    "        self.head_ln = nn.Layernorm((emb_dim))\n",
    "        self.head = nn.Linear(emb_dim, n_emb, bias=False)\n",
    "        self.head.w = self.token_emb.w  # weight tying\n",
    "\n",
    "        self.pos = nn.Buffer(ac.arange(seq_len).view(1, -1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x) + self.pos_emb(self.pos[:, : x.shape[-1]])\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(self.head_ln(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block\"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim, n_heads, mask, dropout, out_scale):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_ln = nn.Layernorm((emb_dim,))\n",
    "        self.attn = nn.MultiHeadSelfAttention(emb_dim, n_heads, mask, dropout)\n",
    "        self.attn.qkv.w.data *= out_scale  # scaling used by GPT-2\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_ln = nn.Layernorm((emb_dim,))\n",
    "        self.mlp = MLP(emb_dim)\n",
    "        self.mlp.down.w.data *= out_scale  # scaling used by GPT-2\n",
    "        self.mlp_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn_dropout(self.attn(self.attn_ln(x)))\n",
    "        x = x + self.mlp_dropout(self.mlp(self.mlp_ln(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed Forward Block\"\"\"\n",
    "\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(n_emb, 4*n_emb)\n",
    "        self.down = nn.Linear(4*n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for causal self-attention\n",
    "mask = ac.full(ctx_len, ctx_len, value=float(\"-inf\")).triu(1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    n_emb=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    seq_len=ctx_len,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_blocks,\n",
    "    mask=mask\n",
    ").to(device)\n",
    "\n",
    "model.n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = nn.Dataloader((X_train, y_train), batch_size, device)\n",
    "val_dl = nn.Dataloader((X_val, y_val), batch_size, device, shuffle_data=False)\n",
    "optim = nn.optimizers.AdamW(model.parameters(), learning_rate=3e-4)\n",
    "val_steps = len(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "step = 1\n",
    "max_steps = 2500\n",
    "val_interval = 250\n",
    "\n",
    "while step <= max_steps:\n",
    "    for x, y in train_dl():\n",
    "        \n",
    "        # training\n",
    "        model.train()\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        loss = F.cross_entropy_loss(model(x), y)\n",
    "        loss.backward()\n",
    "        optim.update_params()\n",
    "        optim.reset_param_grads()\n",
    "\n",
    "        dt = time.perf_counter() - start\n",
    "        tok_per_s = batch_size * ctx_len / dt\n",
    "\n",
    "        # validation\n",
    "        if step > 1 and step % val_interval == 0:\n",
    "            model.eval()\n",
    "            with ac.no_autograd_tracking():\n",
    "                val_loss = 0.0\n",
    "                for x, y in val_dl():\n",
    "                    val_loss += F.cross_entropy_loss(model(x), y).item()\n",
    "                val_loss /= val_steps\n",
    "                print(f\"\\n---\\nval_loss {val_loss:.4f}\\n---\\n\")\n",
    "        \n",
    "        print(f\"step {step}/{max_steps} | loss {loss.item():.4f} | dt {dt:.4f} s | {tok_per_s:.1f} tok/s\")\n",
    "        step += 1\n",
    "\n",
    "        if step > max_steps:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
