{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer: Shakespeare\n",
    "\n",
    "based on: https://github.com/karpathy/build-nanogpt\n",
    "\n",
    "In this example a transformer model is built close to the GPT-2 model. The building blocks are now implemented manually instead of using a `nn.Sequential` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from itertools import cycle\n",
    "\n",
    "import auto_compyute as ac\n",
    "import auto_compyute.nn.functional as F\n",
    "\n",
    "ac.set_random_seed(0)\n",
    "\n",
    "DEVICE = \"cuda\" if ac.gpu_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_LEN = 8  # number of tokens in the input sequence\n",
    "EMB_DIM = 64  # embedding dimension or model dimension\n",
    "N_HEADS = 4  # number of heads in each attention block\n",
    "N_BLOCKS = 6  # number of transformer blocks/layers\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "DATA_URL = (\n",
    "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    ")\n",
    "response = requests.get(DATA_URL, timeout=10)\n",
    "data = response.text\n",
    "\n",
    "# create a character-level tokenizer\n",
    "chars = sorted(set(response.text))\n",
    "vocab = dict(enumerate(chars))\n",
    "ivocab = {c: i for i, c in vocab.items()}\n",
    "\n",
    "\n",
    "def encode(text: str) -> list[int]:\n",
    "    return [ivocab[t] for t in text]\n",
    "\n",
    "\n",
    "def decode(token_ids: list[int]) -> str:\n",
    "    return \"\".join(vocab[id] for id in token_ids)\n",
    "\n",
    "\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "data_enc = ac.tensor(encode(data)).int()\n",
    "\n",
    "# create the datasets. y is just x shifted by one (the next token to predict)\n",
    "n_samples = len(data_enc) // CTX_LEN\n",
    "X = ac.stack(*[data_enc[i * CTX_LEN : i * CTX_LEN + CTX_LEN] for i in range(n_samples)])\n",
    "y = ac.stack(*[data_enc[i * CTX_LEN + 1 : i * CTX_LEN + CTX_LEN + 1] for i in range(n_samples)])\n",
    "\n",
    "print(X[:4])\n",
    "print(y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random train-val split\n",
    "idx = ac.randperm(len(X))\n",
    "n = int(len(X) * 0.9)\n",
    "train_idx, val_idx = idx[:n], idx[n:]\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_val = X[val_idx]\n",
    "y_val = y[val_idx]\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_compyute import nn\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer model following GPT-2\"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, emb_dim, seq_len, n_heads, n_layers, mask, dropout=0):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(n_emb, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, emb_dim)\n",
    "        self.token_emb.w.data *= emb_dim**-0.5  # scaling used by GPT-2\n",
    "        self.pos_emb.w.data *= emb_dim**-0.5  # scaling used by GPT-2\n",
    "\n",
    "        out_scale = (2 * n_layers) ** -0.5  # scaling used by GPT-2\n",
    "        self.blocks = nn.Modulelist(\n",
    "            Block(emb_dim, n_heads, mask, dropout, out_scale) for _ in range(n_layers)\n",
    "        )\n",
    "\n",
    "        self.head_ln = nn.Layernorm((emb_dim))\n",
    "        self.head = nn.Linear(emb_dim, n_emb, bias=False)\n",
    "        self.head.w = self.token_emb.w  # weight tying\n",
    "\n",
    "        self.pos = nn.Buffer(ac.arange(seq_len).view(1, -1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x) + self.pos_emb(self.pos[:, : x.shape[-1]])\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(self.head_ln(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block\"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim, n_heads, mask, dropout, out_scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_ln = nn.Layernorm((emb_dim,))\n",
    "        self.attn = nn.MultiHeadSelfAttention(emb_dim, n_heads, mask, dropout)\n",
    "        self.attn.qkv.w.data *= out_scale  # scaling used by GPT-2\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_ln = nn.Layernorm((emb_dim,))\n",
    "        self.mlp = MLP(emb_dim)\n",
    "        self.mlp.down.w.data *= out_scale  # scaling used by GPT-2\n",
    "        self.mlp_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn_dropout(self.attn(self.attn_ln(x)))\n",
    "        x = x + self.mlp_dropout(self.mlp(self.mlp_ln(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed Forward Block\"\"\"\n",
    "\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.down = nn.Linear(4 * n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for causal self-attention\n",
    "mask = ac.full(CTX_LEN, CTX_LEN, value=float(\"-inf\")).triu(1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    n_emb=vocab_size,\n",
    "    emb_dim=EMB_DIM,\n",
    "    seq_len=CTX_LEN,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_BLOCKS,\n",
    "    mask=mask,\n",
    ").to(DEVICE)\n",
    "\n",
    "model.n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = nn.Dataloader((X_train, y_train), BATCH_SIZE, DEVICE)\n",
    "val_dl = nn.Dataloader((X_val, y_val), BATCH_SIZE, DEVICE, shuffle_data=False)\n",
    "optim = nn.optimizers.AdamW(model.parameters(), learning_rate=3e-4)\n",
    "val_steps = len(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "MAX_STEPS = 2500\n",
    "VAL_INTERVAL = 250\n",
    "\n",
    "step = 1\n",
    "for x, y in cycle(train_dl()):\n",
    "    # training\n",
    "    model.train()\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    loss = F.cross_entropy_loss(model(x), y)\n",
    "    loss.backward()\n",
    "    optim.update_params()\n",
    "    optim.reset_param_grads()\n",
    "\n",
    "    dt = time.perf_counter() - start\n",
    "    tok_per_s = BATCH_SIZE * CTX_LEN / dt\n",
    "\n",
    "    # validation\n",
    "    if step > 1 and step % VAL_INTERVAL == 0:\n",
    "        model.eval()\n",
    "        with ac.no_autograd_tracking():\n",
    "            val_loss = sum(F.cross_entropy_loss(model(x), y).item() for x, y in val_dl())\n",
    "            val_loss /= val_steps\n",
    "            print(f\"\\n---\\nval_loss {val_loss:.4f}\\n---\\n\")\n",
    "\n",
    "    print(f\"{step}/{MAX_STEPS} | loss {loss.item():.4f} | dt {dt:.4f} s | {tok_per_s:.1f} tok/s\")\n",
    "    step += 1\n",
    "\n",
    "    if step > MAX_STEPS:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
